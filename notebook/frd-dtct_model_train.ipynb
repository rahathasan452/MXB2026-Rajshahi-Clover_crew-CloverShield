{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-23T18:34:58.569518Z",
     "iopub.status.busy": "2025-12-23T18:34:58.569272Z",
     "iopub.status.idle": "2025-12-23T18:34:58.839006Z",
     "shell.execute_reply": "2025-12-23T18:34:58.838217Z",
     "shell.execute_reply.started": "2025-12-23T18:34:58.569501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T18:34:58.840595Z",
     "iopub.status.busy": "2025-12-23T18:34:58.840272Z",
     "iopub.status.idle": "2025-12-23T18:35:19.948995Z",
     "shell.execute_reply": "2025-12-23T18:35:19.948254Z",
     "shell.execute_reply.started": "2025-12-23T18:34:58.840574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# STEP 1: INSTALLATION & IMPORTS\n",
    "# ------------------------------------------------------------------\n",
    "# Install required libraries for explainability and graph processing\n",
    "\n",
    "\"\"\"\n",
    "End-to-end fraud pipeline with Train / Val / Test, SHAP explanation, and optional LLM\n",
    "integration for human-readable explanations.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "\"\"\"\n",
    "!pip install groq\n",
    "!pip install -q networkx\n",
    "!pip install xgboost shap openai joblib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "import joblib\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_predict\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, confusion_matrix\n",
    "import warnings\n",
    "#warnings.filterwarnings('default')\n",
    "# Suppress pandas deprecation warnings (these don't affect functionality)\n",
    "warnings.filterwarnings('ignore', message='.*is_sparse.*', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', message='is_sparse is deprecated')\n",
    "warnings.filterwarnings('default', category=UserWarning)  # Still show important user warnings\n",
    "\n",
    "print(\"Step 1: Environment Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T18:35:19.950649Z",
     "iopub.status.busy": "2025-12-23T18:35:19.949812Z",
     "iopub.status.idle": "2025-12-23T18:35:19.958551Z",
     "shell.execute_reply": "2025-12-23T18:35:19.957757Z",
     "shell.execute_reply.started": "2025-12-23T18:35:19.950625Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# STEP 2: DATA OPTIMIZATION\n",
    "# ------------------------------------------------------------------\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" Iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage. \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32) # float16 has lower precision, using 32\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage decreased to {end_mem:.2f} MB ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T18:35:19.960439Z",
     "iopub.status.busy": "2025-12-23T18:35:19.960188Z",
     "iopub.status.idle": "2025-12-23T18:35:32.464565Z",
     "shell.execute_reply": "2025-12-23T18:35:32.463744Z",
     "shell.execute_reply.started": "2025-12-23T18:35:19.960418Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load Dataset (Assuming file is in Kaggle input directory)\n",
    "# NOTE: PaySim dataset is usually at /kaggle/input/paysim1/PS_20174392719_1491204439457_log.csv\n",
    "# Adjust path if necessary.\n",
    "file_path = '/kaggle/input/paysim1/PS_20174392719_1491204439457_log.csv'\n",
    "\n",
    "# Read CSV\n",
    "df = pd.read_csv(file_path)\n",
    "df = reduce_mem_usage(df)\n",
    "\n",
    "# Rename columns for clarity\n",
    "df = df.rename(columns={'oldbalanceOrg':'oldBalanceOrig', 'newbalanceOrig':'newBalanceOrig', \n",
    "                        'oldbalanceDest':'oldBalanceDest', 'newbalanceDest':'newBalanceDest'})\n",
    "\n",
    "print(f\"Step 2: Data Loaded. Shape: {df.shape}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T18:35:32.465312Z",
     "iopub.status.busy": "2025-12-23T18:35:32.465132Z",
     "iopub.status.idle": "2025-12-23T18:35:33.554929Z",
     "shell.execute_reply": "2025-12-23T18:35:33.554245Z",
     "shell.execute_reply.started": "2025-12-23T18:35:32.465298Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T18:35:33.556080Z",
     "iopub.status.busy": "2025-12-23T18:35:33.555856Z",
     "iopub.status.idle": "2025-12-23T18:35:33.840171Z",
     "shell.execute_reply": "2025-12-23T18:35:33.839592Z",
     "shell.execute_reply.started": "2025-12-23T18:35:33.556064Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.groupby('type')['isFraud'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T18:35:33.841073Z",
     "iopub.status.busy": "2025-12-23T18:35:33.840867Z",
     "iopub.status.idle": "2025-12-23T18:35:34.186411Z",
     "shell.execute_reply": "2025-12-23T18:35:34.185731Z",
     "shell.execute_reply.started": "2025-12-23T18:35:33.841055Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T18:35:34.187371Z",
     "iopub.status.busy": "2025-12-23T18:35:34.187124Z",
     "iopub.status.idle": "2025-12-23T18:35:49.937329Z",
     "shell.execute_reply": "2025-12-23T18:35:49.936522Z",
     "shell.execute_reply.started": "2025-12-23T18:35:34.187346Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T18:35:49.938263Z",
     "iopub.status.busy": "2025-12-23T18:35:49.938062Z",
     "iopub.status.idle": "2025-12-23T18:35:49.956269Z",
     "shell.execute_reply": "2025-12-23T18:35:49.955127Z",
     "shell.execute_reply.started": "2025-12-23T18:35:49.938249Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1) FEATURE ENGINEER (Fitted inside pipeline)\n",
    "# ----------------------------\n",
    "class FraudFeatureEngineer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Vectorized, deterministic feature transformer.\n",
    "    - Builds weighted directed graph (aggregated by (origin,dest) counts)\n",
    "    - Creates frequency, ratio, log and graph features\n",
    "    \"\"\"\n",
    "    def __init__(self, pagerank_limit=None):\n",
    "        self.pagerank_limit = pagerank_limit\n",
    "        self.stats = {}\n",
    "        self.graph_meta = {}\n",
    "        self.type_map = {'TRANSFER': 0, 'CASH_OUT': 1}\n",
    "        self.global_mean = 0.0\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        # Sort by time if available for deterministic graphs\n",
    "        if 'step' in X.columns:\n",
    "            X = X.sort_values('step')\n",
    "\n",
    "        # Basic global stat\n",
    "        self.global_mean = float(X['amount'].mean())\n",
    "        self.global_median = float(X['amount'].median())\n",
    "\n",
    "        # Frequency & mean\n",
    "        self.stats['orig_counts'] = X['nameOrig'].value_counts().to_dict()\n",
    "        self.stats['dest_counts'] = X['nameDest'].value_counts().to_dict()\n",
    "        self.stats['orig_mean_amt'] = X.groupby('nameOrig')['amount'].mean().to_dict()\n",
    "        self.stats['orig_median_amt'] = (X.groupby('nameOrig')['amount'].median().to_dict())\n",
    "        self.stats['orig_log_median_amt'] = (X.groupby('nameOrig')['amount'].apply(lambda s: np.log1p(s).median()).to_dict())\n",
    "        self.stats['last_step'] = X.groupby('nameOrig')['step'].last().to_dict()\n",
    "\n",
    "        # Weighted graph: count transactions per (origin,dest)\n",
    "        edge_weights = X.groupby(['nameOrig', 'nameDest']).size().reset_index(name='weight')\n",
    "        G = nx.DiGraph()\n",
    "        if not edge_weights.empty:\n",
    "            edges = [(r['nameOrig'], r['nameDest'], float(r['weight'])) for _, r in edge_weights.iterrows()]\n",
    "            G.add_weighted_edges_from(edges, weight='weight')\n",
    "\n",
    "        self.graph_meta['in_degree'] = dict(G.in_degree(weight='weight'))\n",
    "        self.graph_meta['out_degree'] = dict(G.out_degree(weight='weight'))\n",
    "\n",
    "    \n",
    "        # Pagerank: limit nodes if requested to save time/memory\n",
    "        try:\n",
    "            if G.number_of_nodes() == 0:\n",
    "                self.graph_meta['pagerank'] = {}\n",
    "            elif self.pagerank_limit and self.pagerank_limit < G.number_of_nodes():\n",
    "                top_nodes = sorted(G.degree(weight='weight'), key=lambda x: x[1], reverse=True)[:self.pagerank_limit]\n",
    "                keep = set(n for n, _ in top_nodes)\n",
    "                sub = G.subgraph(keep).copy()\n",
    "                self.graph_meta['pagerank'] = nx.pagerank(sub, alpha=0.85, weight='weight', tol=1e-4)\n",
    "            else:\n",
    "                self.graph_meta['pagerank'] = nx.pagerank(G, alpha=0.85, weight='weight', tol=1e-4)\n",
    "        except Exception:\n",
    "            # Pagerank failure should not break training\n",
    "            self.graph_meta['pagerank'] = {}\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "\n",
    "        # Time features\n",
    "        X['hour'] = X['step'] % 24 if 'step' in X.columns else 0\n",
    "        '''\n",
    "        # Balance-check features\n",
    "        X['errorBalanceOrig'] = X['newBalanceOrig'].fillna(0) + X['amount'].fillna(0) - X['oldBalanceOrig'].fillna(0)\n",
    "        X['errorBalanceDest'] = X['oldBalanceDest'].fillna(0) + X['amount'].fillna(0) - X['newBalanceDest'].fillna(0)\n",
    "        '''\n",
    "        # Frequency mapping\n",
    "        X['orig_txn_count'] = X['nameOrig'].map(self.stats.get('orig_counts', {})).fillna(0).astype(int)\n",
    "        X['dest_txn_count'] = X['nameDest'].map(self.stats.get('dest_counts', {})).fillna(0).astype(int)\n",
    "\n",
    "        # Ratio features\n",
    "        user_mean = X['nameOrig'].map(self.stats.get('orig_mean_amt', {})).fillna(self.global_mean)\n",
    "        X['amt_ratio_to_user_mean'] = X['amount'] / (user_mean + 1.0)\n",
    "        X['amount_log1p'] = np.log1p(X['amount'])\n",
    "        X['amount_over_oldBalanceOrig'] = X['amount'] / (X['oldBalanceOrig'].replace(0, np.nan).fillna(1.0))\n",
    "        \n",
    "        user_median = X['nameOrig'].map(self.stats.get('orig_median_amt', {})).fillna(self.global_median)\n",
    "        # Apply fallback to global median for users with too few transactions\n",
    "        MIN_TXNS = 3\n",
    "        user_median = np.where(X['orig_txn_count'] >= MIN_TXNS, user_median, self.global_median)\n",
    "        X['amt_ratio_to_user_median'] = (X['amount'] / (user_median + 1.0))\n",
    "        \n",
    "        user_log_median = X['nameOrig'].map(self.stats['orig_log_median_amt']).fillna(np.log1p(self.global_median))\n",
    "        X['amt_log_ratio_to_user_median'] = (np.log1p(X['amount']) / (user_log_median + 1e-6))\n",
    "\n",
    "\n",
    "        \n",
    "        # Graph features\n",
    "        X['in_degree'] = X['nameDest'].map(self.graph_meta.get('in_degree', {})).fillna(0).astype(float)\n",
    "        X['out_degree'] = X['nameOrig'].map(self.graph_meta.get('out_degree', {})).fillna(0).astype(float)\n",
    "        X['network_trust'] = X['nameOrig'].map(self.graph_meta.get('pagerank', {})).fillna(0.0).astype(float)\n",
    "\n",
    "        # New/novelty flags\n",
    "        X['is_new_origin'] = (X['orig_txn_count'] == 0).astype(int)\n",
    "        X['is_new_dest'] = (X['dest_txn_count'] == 0).astype(int)\n",
    "\n",
    "        # Type encoding (fast & vectorized)\n",
    "        X['type_encoded'] = X['type'].map(self.type_map).fillna(-1).astype(int)\n",
    "\n",
    "        # Drop identifiers and non-numeric columns\n",
    "        for c in ['nameOrig', 'nameDest', 'type', 'isFlaggedFraud']:\n",
    "            if c in X.columns:\n",
    "                X.drop(columns=c, inplace=True)\n",
    "\n",
    "        # Return numeric-only DataFrame expected by XGBoost & SHAP\n",
    "        return X.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T18:35:49.958507Z",
     "iopub.status.busy": "2025-12-23T18:35:49.958117Z",
     "iopub.status.idle": "2025-12-23T18:35:49.985108Z",
     "shell.execute_reply": "2025-12-23T18:35:49.984222Z",
     "shell.execute_reply.started": "2025-12-23T18:35:49.958482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2) Split helpers: test temporal, train/test only \n",
    "# ----------------------------\n",
    "def make_splits(df, test_frac=0.05, time_col='step', min_test_fraud=100, random_state=42):\n",
    "    \"\"\"\n",
    "    Returns: X_train, X_test, y_train, y_test, time_used\n",
    "    - Test is time-based (last test_frac proportion) if `time_col` exists.\n",
    "    - Otherwise a stratified test split is used.\n",
    "    - If temporal test exists but contains < min_test_fraud frauds, fallback to stratified test split.\n",
    "    \"\"\"\n",
    "    # Temporal test split\n",
    "    if time_col in df.columns:\n",
    "        cutoff = df[time_col].quantile(1 - test_frac)\n",
    "        train_df = df[df[time_col] <= cutoff].reset_index(drop=True)\n",
    "        test_df = df[df[time_col] > cutoff].reset_index(drop=True)\n",
    "    else:\n",
    "        train_df = df.copy()\n",
    "        test_df = pd.DataFrame(columns=df.columns)  # empty, fallback\n",
    "\n",
    "    # If test is too small or contains too few frauds, fallback to stratified split on entire dataset\n",
    "    if (test_df.empty) or (test_df['isFraud'].sum() < min_test_fraud):\n",
    "        X_temp = df.drop(columns=['isFraud'])\n",
    "        y_temp = df['isFraud']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_temp, y_temp, test_size=test_frac, stratify=y_temp, random_state=random_state\n",
    "        )\n",
    "        return X_train.reset_index(drop=True), X_test.reset_index(drop=True), y_train.reset_index(drop=True), y_test.reset_index(drop=True), False\n",
    "    else:\n",
    "        X_train = train_df.drop(columns=['isFraud'])\n",
    "        y_train = train_df['isFraud']\n",
    "        X_test = test_df.drop(columns=['isFraud'])\n",
    "        y_test = test_df['isFraud']\n",
    "        return X_train.reset_index(drop=True), X_test.reset_index(drop=True), y_train.reset_index(drop=True), y_test.reset_index(drop=True), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T18:35:49.986278Z",
     "iopub.status.busy": "2025-12-23T18:35:49.985983Z",
     "iopub.status.idle": "2025-12-23T18:35:50.010136Z",
     "shell.execute_reply": "2025-12-23T18:35:50.009219Z",
     "shell.execute_reply.started": "2025-12-23T18:35:49.986256Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3) Training function: train on TRAIN, test on TEST \n",
    "# ----------------------------\n",
    "def train_pipeline(df,\n",
    "                   pagerank_limit=100000,\n",
    "                   random_state=42,\n",
    "                   search_iter=3,\n",
    "                   target_recall=0.99):\n",
    "    \"\"\"\n",
    "    Full training flow adapted to only use TRAIN and TEST:\n",
    "      - splits data (train / test; temporal test when possible)\n",
    "      - randomized search on TRAIN only (with internal CV)\n",
    "      - selects threshold using cross-validated predictions on TRAIN\n",
    "      - refits final pipeline on TRAIN (no early stopping, since no val)\n",
    "      - returns final_pipeline, optimal_threshold, and test set for evaluation\n",
    "    \"\"\"\n",
    "    # Prepare splits (now returns X_train, X_test, y_train, y_test, time_used)\n",
    "    X_train, X_test, y_train, y_test, time_used = make_splits(\n",
    "        df, test_frac=0.05, time_col='step', random_state=random_state\n",
    "    )\n",
    "\n",
    "    print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}; time_split_used={time_used}\")\n",
    "    print(\"Fraud counts -> train:\", int(y_train.sum()), \"test:\", int(y_test.sum()))\n",
    "\n",
    "\n",
    "    # Build pipeline objects\n",
    "    fe = FraudFeatureEngineer(pagerank_limit=pagerank_limit)\n",
    "    #-----trnsOutSrch\n",
    "    print(\"\\n[Step A] Running Feature Engineering on FULL Training Set (Once)...\") \n",
    "    # Fit and transform the raw X_train once\n",
    "    X_train_search = fe.fit_transform(X_train, y_train)\n",
    "    #-----trnsOutSrch\n",
    "    xgb_base = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                                 tree_method=\"hist\",\n",
    "                                 device='cuda',\n",
    "                                 n_jobs=1,  # keep job control to outer search\n",
    "                                 random_state=random_state,\n",
    "                                 use_label_encoder=False,\n",
    "                                 verbosity=1)\n",
    "\n",
    "    pipeline = Pipeline([('fe', fe), ('clf', xgb_base)])\n",
    "\n",
    "    # Heuristic scale_pos_weight (from train)\n",
    "    neg = (y_train == 0).sum()\n",
    "    pos = (y_train == 1).sum()\n",
    "    spw = int(max(1, neg / max(1, pos)))  #scale_pos_weight\n",
    "\n",
    "    param_dist = {\n",
    "        'n_estimators': randint(300, 900),\n",
    "        'max_depth': randint(4, 8),\n",
    "        'learning_rate': uniform(0.03, 0.12),\n",
    "        'scale_pos_weight': [spw],\n",
    "        'subsample': uniform(0.7, 0.3),\n",
    "        'colsample_bytree': uniform(0.6, 0.4)\n",
    "    }\n",
    "    \n",
    "    \"\"\"\n",
    "    param_dist = {\n",
    "    # Number of trees\n",
    "    'clf__n_estimators': randint(300, 900),\n",
    "    # Tree depth (PaySim works best shallow–medium)\n",
    "    'clf__max_depth': randint(4, 8),\n",
    "    # Learning rate (avoid very small, too slow)\n",
    "    'clf__learning_rate': uniform(0.03, 0.12),\n",
    "    # Imbalance handling (keep fixed or narrow)\n",
    "    'clf__scale_pos_weight': [spw],\n",
    "    # Row sampling (important for generalization)\n",
    "    'clf__subsample': uniform(0.7, 0.3),\n",
    "    # Column sampling (often overlooked, very important)\n",
    "    'clf__colsample_bytree': uniform(0.6, 0.4)\n",
    "    }\n",
    "    \"\"\"\n",
    "    ######\n",
    "    ####\n",
    "    ###-----trnsOutSrch srch optmize .  pipeline --> xgb_base  X_train --> X_train_search\n",
    "    search = RandomizedSearchCV(xgb_base, param_distributions=param_dist, n_iter=search_iter,\n",
    "                                scoring='average_precision', cv=3, verbose=2, n_jobs=-1, random_state=random_state,\n",
    "                                refit=True)\n",
    "    # Fit search on TRAIN only\n",
    "    print(f\"\\nStarting RandomizedSearchCV: {search_iter} candidates × 3 folds = {search_iter * 3} model fits\")\n",
    "    print(\"This may take several minutes depending on dataset size. Progress will be shown below...\\n\")\n",
    "    search.fit(X_train_search, y_train)\n",
    "    print(\"\\n\\nRandom search best params:\", search.best_params_)\n",
    "\n",
    "    \n",
    "    # ---------- Threshold selection using cross-validated predictions on TRAIN ----------\n",
    "    # Use cross_val_predict to get out-of-fold probabilities on training data (keeps test untouched)\n",
    "    try:\n",
    "        train_probs_cv = cross_val_predict(\n",
    "            search.best_estimator_, X_train_search, y_train, cv=3,\n",
    "            method='predict_proba', n_jobs=2\n",
    "        )[:, 1]\n",
    "    except Exception:\n",
    "        # Fallback: use the fitted pipeline's predict_proba on X_train (risk of optimistic threshold)\n",
    "        train_probs_cv = search.predict_proba(X_train_search)[:, 1]\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_train, train_probs_cv)\n",
    "    idxs = np.where(recall[:-1] >= target_recall)[0]\n",
    "    if len(idxs) > 0:\n",
    "        optimal_threshold = float(thresholds[idxs[-1]])  # largest threshold preserving recall\n",
    "    else:\n",
    "        optimal_threshold = float(thresholds[0]) if len(thresholds) > 0 else 0.2\n",
    "\n",
    "    print(f\"Chosen threshold from TRAIN (CV) : {optimal_threshold:.8f} (target recall {target_recall})\")\n",
    "\n",
    "    # Optionally print training performance at chosen threshold (out-of-fold)\n",
    "    train_preds_cv = (train_probs_cv >= optimal_threshold).astype(int)\n",
    "    print(\"\\nTraining (CV) performance at chosen threshold:\")\n",
    "    print(classification_report(y_train, train_preds_cv))\n",
    "    print(\"Train Confusion Matrix:\\n\", confusion_matrix(y_train, train_preds_cv))\n",
    "\n",
    "    # ---------- Refit final model on full TRAIN ----------\n",
    "    # Fit a fresh feature engineer on entire train\n",
    "    fe_final = FraudFeatureEngineer(pagerank_limit=pagerank_limit)\n",
    "    fe_final.fit(X_train, y_train)\n",
    "\n",
    "    X_train_trans = fe_final.transform(X_train)\n",
    "    #X_test_trans = fe_final.transform(X_test) if len(X_test) > 0 else None\n",
    "    print(\"\\nSample of X_train_trans:\")\n",
    "    print(X_train_trans.sample(10))\n",
    "\n",
    "\n",
    "    # Build classifier with best params (no early stopping since no val set)\n",
    "    #cls_params = {k.replace('clf__', ''): v for k, v in search.best_params_.items() if k.startswith('clf__')}\n",
    "    cls_params =search.best_params_\n",
    "    clf_final = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                                  tree_method=\"hist\",\n",
    "                                  device='cuda',\n",
    "                                  n_jobs=1,\n",
    "                                  random_state=random_state,\n",
    "                                  use_label_encoder=False,\n",
    "                                  verbosity=2,\n",
    "                                  **cls_params)\n",
    "\n",
    "    clf_final.fit(X_train_trans, y_train)\n",
    "\n",
    "    # Final pipeline contains fitted FE and fitted classifier\n",
    "    final_pipeline = Pipeline([('fe', fe_final), ('clf', clf_final)])\n",
    "    # saving pkl as only clf xgb w/o fe to reduce model size .. and render dont run out of ram (512mb limit)\n",
    "    clf = final_pipeline.named_steps['clf']\n",
    "    joblib.dump(clf, 'fraud_pipeline_final.pkl')\n",
    "    #\n",
    "    ##----refit=True\n",
    "    ###final_pipeline = Pipeline([('fe', fe), ('clf', search.best_estimator_)])\n",
    "    # Save ONLY the classifier step, not the Feature Engineering step\n",
    "\n",
    "    '''\n",
    "    # Save pipeline artifact\n",
    "    joblib.dump(final_pipeline, 'fraud_pipeline_final.pkl', compress=9)\n",
    "    print(\"Saved final pipeline to 'fraud_pipeline_final.pkl'\")\n",
    "    '''\n",
    "    # Return pipeline, threshold and test partitions (raw df forms for later inference)\n",
    "    return final_pipeline, optimal_threshold, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2025-12-23T18:35:50.011614Z",
     "iopub.status.busy": "2025-12-23T18:35:50.011301Z",
     "iopub.status.idle": "2025-12-23T18:35:50.027991Z",
     "shell.execute_reply": "2025-12-23T18:35:50.026689Z",
     "shell.execute_reply.started": "2025-12-23T18:35:50.011590Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4) SHAP + LLM explain function and prediction wrapper\n",
    "# ----------------------------\n",
    "def predict_and_explain(pipeline, transaction_df, threshold, shap_background=None, topk=6, llm_api_key=None):\n",
    "    \"\"\"\n",
    "    - pipeline: fitted sklearn Pipeline with steps 'fe' and 'clf'\n",
    "    - transaction_df: raw transaction(s) DataFrame (same columns used in training)\n",
    "    - threshold: numeric threshold chosen from validation\n",
    "    - shap_background: optional background DataFrame for SHAP explainer (raw, not transformed).\n",
    "                       If None, the function will attempt to sample from pipeline.fe.stats if available.\n",
    "    - topk: how many top contributors to include in the explanation\n",
    "    - llm_api_key: if provided, function will call Groq API to generate a textual explanation.\n",
    "\n",
    "    Returns: dict with keys:\n",
    "      'probabilities', 'decisions', 'shap_table' (DataFrame), 'llm_explanation' (str or None)\n",
    "    \"\"\"\n",
    "    # Predict probabilities & binary decision\n",
    "    probs = pipeline.predict_proba(transaction_df)[:, 1]\n",
    "    decisions = (probs >= threshold).astype(int)\n",
    "\n",
    "    # Prepare features for SHAP (transformed numeric features)\n",
    "    fe = pipeline.named_steps['fe']\n",
    "    clf = pipeline.named_steps['clf']\n",
    "\n",
    "    X_trans = fe.transform(transaction_df)  # numeric matrix with columns in fixed order\n",
    "    # Determine background for SHAP: prefer provided, else use fe.stats to synthesize small background\n",
    "    if shap_background is None:\n",
    "        # If we have access to saved stats, try to build a tiny synthetic background sample.\n",
    "        # Fallback: use the transaction itself repeated (not ideal but safe).\n",
    "        try:\n",
    "            # if fe.stats contains 'orig_counts', we might not have raw rows; fallback above\n",
    "            shap_background_trans = X_trans.iloc[[0]].copy()\n",
    "        except Exception:\n",
    "            shap_background_trans = X_trans.iloc[[0]].copy()\n",
    "    else:\n",
    "        # transform provided background raw df\n",
    "        shap_background_trans = fe.transform(shap_background)\n",
    "\n",
    "    # Create SHAP explainer\n",
    "    try:\n",
    "        # shap.Explainer is model-agnostic and often handles sklearn wrappers well\n",
    "        explainer = shap.Explainer(clf, shap_background_trans, feature_names=X_trans.columns.tolist())\n",
    "        shap_exp = explainer(X_trans)  # Explanation object\n",
    "        # shap_exp.values shape: (n_samples, n_features)\n",
    "        shap_vals = shap_exp.values[0] if shap_exp.values.ndim == 2 else shap_exp.values\n",
    "        feature_names = X_trans.columns.tolist()\n",
    "    except Exception:\n",
    "        # Last-resort fallback using TreeExplainer on the raw booster - works for xgboost\n",
    "        try:\n",
    "            explainer = shap.TreeExplainer(clf)\n",
    "            shap_vals = explainer.shap_values(X_trans)[0]\n",
    "            feature_names = X_trans.columns.tolist()\n",
    "        except Exception:\n",
    "            # Unable to compute SHAP; return empty table\n",
    "            shap_vals = np.zeros(X_trans.shape[1])\n",
    "            feature_names = X_trans.columns.tolist()\n",
    "\n",
    "    # Build a DataFrame of feature contributions\n",
    "    feat_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'value': X_trans.iloc[0].values,\n",
    "        'shap_abs': np.abs(shap_vals),\n",
    "        'shap': shap_vals\n",
    "    })\n",
    "    feat_df = feat_df.sort_values('shap_abs', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Prepare LLM prompt summarizing top contributors\n",
    "    top_feats = feat_df.head(topk)[['feature', 'value', 'shap']].copy()\n",
    "    top_lines = []\n",
    "    for _, row in top_feats.iterrows():\n",
    "        top_lines.append(f\"- {row['feature']}: value={row['value']:.6g}, shap={row['shap']:.6g}\")\n",
    "\n",
    "    llm_explanation_text = None\n",
    "    if llm_api_key:\n",
    "        try:\n",
    "            from groq import Groq  # <--- CHANGED: Import Groq instead of openai\n",
    "            \n",
    "            client = Groq(api_key=llm_api_key) # <--- CHANGED: Initialize Groq client\n",
    "            \n",
    "            system_prompt = (\n",
    "                \"You are a concise fraud-analytics assistant. \"\n",
    "                \"Given feature contributions (SHAP values) and their observed values for a single transaction, \"\n",
    "                \"produce a short (3-6 sentences) human-readable explanation why the model assigned the given fraud probability. \"\n",
    "                \"Mention which factors increase or decrease risk, and a brief recommended action (e.g., block / review / allow).\"\n",
    "            )\n",
    "            user_prompt = (\n",
    "                f\"Model fraud probability: {probs[0]:.4f}\\n\"\n",
    "                f\"Threshold for blocking: {threshold:.4f}\\n\"\n",
    "                f\"Top contributing features (feature: value, shap):\\n\" + \"\\n\".join(top_lines) +\n",
    "                \"\\n\\nWrite the short explanation now.\"\n",
    "            )\n",
    "\n",
    "            # <--- CHANGED: Call Groq's chat.completions with a free Llama 3 model\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                model = \"llama-3.1-8b-instant\"  , # <--- CHANGED: Use free 'llama3-8b-8192' model\n",
    "                temperature=0.0,\n",
    "                max_tokens=250\n",
    "            )\n",
    "            \n",
    "            # <--- CHANGED: Access response content (structure is same as OpenAI)\n",
    "            llm_explanation_text = chat_completion.choices[0].message.content.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            llm_explanation_text = f\"(LLM generation failed: {str(e)})\"\n",
    "    # Return structured outputs\n",
    "    return {\n",
    "        'probabilities': probs,\n",
    "        'decisions': decisions,\n",
    "        'shap_table': feat_df,\n",
    "        'llm_explanation': llm_explanation_text\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2025-12-23T18:35:50.029814Z",
     "iopub.status.busy": "2025-12-23T18:35:50.029543Z",
     "iopub.status.idle": "2025-12-23T19:06:54.126822Z",
     "shell.execute_reply": "2025-12-23T19:06:54.125744Z",
     "shell.execute_reply.started": "2025-12-23T18:35:50.029791Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5) Example usage: train, then predict and explain on one sample\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset (adjust path as needed)\n",
    "    # Adjust path if necessary.\n",
    "    file_path = '/kaggle/input/paysim1/PS_20174392719_1491204439457_log.csv'\n",
    "\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = reduce_mem_usage(df)\n",
    "    df = df.rename(columns={'oldbalanceOrg':'oldBalanceOrig', 'newbalanceOrig':'newBalanceOrig',\n",
    "                            'oldbalanceDest':'oldBalanceDest', 'newbalanceDest':'newBalanceDest'})\n",
    "    df = df[df['type'].isin(['TRANSFER', 'CASH_OUT'])].reset_index(drop=True)\n",
    "\n",
    "    # Train pipeline (this will take some time depending on data size)\n",
    "    pipeline, threshold, X_test_raw, y_test = train_pipeline(df, pagerank_limit=100000,\n",
    "                                                            random_state=42, search_iter=6,\n",
    "                                                            target_recall=0.99)\n",
    "\n",
    "    # Example single new transaction (raw columns same as training raw df)\n",
    "    new_transaction = pd.DataFrame([{\n",
    "        'step': df['step'].max() + 1,\n",
    "        'type': 'CASH_OUT',\n",
    "        'amount': 500000.0,\n",
    "        'nameOrig': 'C12345_NEW_USER',\n",
    "        'oldBalanceOrig': 500000.0,\n",
    "        'newBalanceOrig': 0.0,\n",
    "        'nameDest': 'C99999_EXISTING_BAD',\n",
    "        'oldBalanceDest': 0.0,\n",
    "        'newBalanceDest': 0.0,\n",
    "        'isFlaggedFraud': 0\n",
    "    }])\n",
    "\n",
    "    # Optionally supply a small background sample for SHAP (raw rows from training)\n",
    "    # Here we sample 200 rows from the original training area if available\n",
    "    shap_bg = None\n",
    "    try:\n",
    "        # if we have a test partition raw DataFrame, use some rows from it as background (or from df earlier)\n",
    "        shap_bg = df.sample(n=200, random_state=42).drop(columns=['isFraud'])\n",
    "    except Exception:\n",
    "        shap_bg = None\n",
    "\n",
    "    # If you want LLM textual explanations, provide GROQ_API_KEY environment variable or pass into function\n",
    "    # IMPORTANT: Never commit API keys to version control. Use environment variables only.\n",
    "    GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "    # Predict and explain\n",
    "    # <--- CHANGED: Pass 'llm_api_key' instead of 'openai_api_key'\n",
    "    result = predict_and_explain(pipeline, new_transaction, threshold, shap_background=shap_bg,\n",
    "                                 topk=6, llm_api_key=GROQ_API_KEY)\n",
    "    \n",
    "    print(f\"Fraud probability: {result['probabilities'][0]:.4f}\")\n",
    "    print(f\"Decision (threshold {threshold:.4f}): {'BLOCK' if result['decisions'][0] == 1 else 'ALLOW'}\")\n",
    "    print(\"\\nTop SHAP contributors:\")\n",
    "    print(result['shap_table'].head(10).to_string(index=False))\n",
    "\n",
    "    if result['llm_explanation']:\n",
    "        print(\"\\nLLM Explanation:\")\n",
    "        print(result['llm_explanation'])\n",
    "    else:\n",
    "        print(\"\\nNo LLM explanation generated (provide GROQ_API_KEY environment variable to enable).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T19:06:54.128109Z",
     "iopub.status.busy": "2025-12-23T19:06:54.127840Z",
     "iopub.status.idle": "2025-12-23T19:06:54.780512Z",
     "shell.execute_reply": "2025-12-23T19:06:54.779568Z",
     "shell.execute_reply.started": "2025-12-23T19:06:54.128086Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import joblib\n",
    "X_train, X_test_raw, y_train, y_test, time_used = make_splits(\n",
    "        df, test_frac=0.05, time_col='step', random_state=42)\n",
    "'''\n",
    "# IMPORTANT: Never commit API keys to version control. Use environment variables only.\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "pipeline = joblib.load(\"/kaggle/input/frddtctmodel1/fraud_pipeline_final (1).pkl\")\n",
    "\n",
    "threshold = 0.0793  # reuse training threshold\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T19:06:54.781521Z",
     "iopub.status.busy": "2025-12-23T19:06:54.781270Z",
     "iopub.status.idle": "2025-12-23T19:07:49.247201Z",
     "shell.execute_reply": "2025-12-23T19:07:49.246233Z",
     "shell.execute_reply.started": "2025-12-23T19:06:54.781504Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result = predict_and_explain(pipeline, X_test_raw, threshold, shap_background=None,topk=4, llm_api_key=GROQ_API_KEY)\n",
    "print(\"\\nTEST performance at TRAIN-chosen threshold:\")\n",
    "print(classification_report(y_test, result['decisions']))\n",
    "print(\"Test Confusion Matrix:\\n\",confusion_matrix(y_test, result['decisions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T19:07:49.248279Z",
     "iopub.status.busy": "2025-12-23T19:07:49.248024Z",
     "iopub.status.idle": "2025-12-23T19:07:50.132762Z",
     "shell.execute_reply": "2025-12-23T19:07:50.132034Z",
     "shell.execute_reply.started": "2025-12-23T19:07:49.248257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_test_raw.to_csv(\"/kaggle/working/test_dataset.csv\", index=True)\n",
    "\n",
    "print(\"Saved to /kaggle/working/test_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T19:07:50.134049Z",
     "iopub.status.busy": "2025-12-23T19:07:50.133736Z",
     "iopub.status.idle": "2025-12-23T19:07:50.390968Z",
     "shell.execute_reply": "2025-12-23T19:07:50.389958Z",
     "shell.execute_reply.started": "2025-12-23T19:07:50.134033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls -lh /kaggle/working\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1069,
     "sourceId": 1940,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9077750,
     "sourceId": 14229262,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 542261,
     "modelInstanceId": 528214,
     "sourceId": 696428,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
