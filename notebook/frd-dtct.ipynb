{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1940,"sourceType":"datasetVersion","datasetId":1069},{"sourceId":14229262,"sourceType":"datasetVersion","datasetId":9077750}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-19T23:59:36.258075Z","iopub.execute_input":"2025-12-19T23:59:36.258404Z","iopub.status.idle":"2025-12-19T23:59:36.266131Z","shell.execute_reply.started":"2025-12-19T23:59:36.258382Z","shell.execute_reply":"2025-12-19T23:59:36.265403Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/paysim1/PS_20174392719_1491204439457_log.csv\n/kaggle/input/frddtctmodel1/fraud_pipeline_final (1).pkl\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ------------------------------------------------------------------\n# STEP 1: INSTALLATION & IMPORTS\n# ------------------------------------------------------------------\n# Install required libraries for explainability and graph processing\n\n\"\"\"\nEnd-to-end fraud pipeline with Train / Val / Test, SHAP explanation, and optional LLM\nintegration for human-readable explanations.\n\nRequirements:\n\n\"\"\"\n!pip install groq\n!pip install -q networkx\n!pip install xgboost shap openai joblib\nimport os\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nimport xgboost as xgb\nimport shap\nimport joblib\nfrom scipy.stats import randint, uniform\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_predict\nfrom sklearn.metrics import classification_report, precision_recall_curve, confusion_matrix\nimport warnings\n#warnings.filterwarnings('default')\n# Suppress pandas deprecation warnings (these don't affect functionality)\nwarnings.filterwarnings('ignore', message='.*is_sparse.*', category=FutureWarning)\nwarnings.filterwarnings('ignore', message='is_sparse is deprecated')\nwarnings.filterwarnings('default', category=UserWarning)  # Still show important user warnings\n\nprint(\"Step 1: Environment Ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T23:59:40.987150Z","iopub.execute_input":"2025-12-19T23:59:40.987736Z","iopub.status.idle":"2025-12-19T23:59:50.512920Z","shell.execute_reply.started":"2025-12-19T23:59:40.987711Z","shell.execute_reply":"2025-12-19T23:59:50.511939Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (1.0.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.11.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.12.4)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\nRequirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.15.0)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\nRequirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.0.3)\nRequirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.44.1)\nRequirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (2.7.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.2.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.3)\nRequirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\nRequirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (25.0)\nRequirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.7)\nRequirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.2)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.11.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.12.4)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.15.0)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->shap) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->xgboost) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.6.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xgboost) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xgboost) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->xgboost) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->xgboost) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->xgboost) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->xgboost) (2024.2.0)\nStep 1: Environment Ready.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ------------------------------------------------------------------\n# STEP 2: DATA OPTIMIZATION\n# ------------------------------------------------------------------\ndef reduce_mem_usage(df):\n    \"\"\" Iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage. \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32) # float16 has lower precision, using 32\n                else:\n                    df[col] = df[col].astype(np.float32)\n    end_mem = df.memory_usage().sum() / 1024**2\n    print(f'Memory usage decreased to {end_mem:.2f} MB ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n    return df\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T00:00:02.933826Z","iopub.execute_input":"2025-12-20T00:00:02.934104Z","iopub.status.idle":"2025-12-20T00:00:02.954455Z","shell.execute_reply.started":"2025-12-20T00:00:02.934089Z","shell.execute_reply":"2025-12-20T00:00:02.953656Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Load Dataset (Assuming file is in Kaggle input directory)\n# NOTE: PaySim dataset is usually at /kaggle/input/paysim1/PS_20174392719_1491204439457_log.csv\n# Adjust path if necessary.\nfile_path = '/kaggle/input/paysim1/PS_20174392719_1491204439457_log.csv'\n\n# Read CSV\ndf = pd.read_csv(file_path)\ndf = reduce_mem_usage(df)\n\n# Rename columns for clarity\ndf = df.rename(columns={'oldbalanceOrg':'oldBalanceOrig', 'newbalanceOrig':'newBalanceOrig', \n                        'oldbalanceDest':'oldBalanceDest', 'newbalanceDest':'newBalanceDest'})\n\nprint(f\"Step 2: Data Loaded. Shape: {df.shape}\")\ndf.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T23:59:52.299755Z","iopub.execute_input":"2025-12-19T23:59:52.300041Z","iopub.status.idle":"2025-12-20T00:00:02.924196Z","shell.execute_reply.started":"2025-12-19T23:59:52.300020Z","shell.execute_reply":"2025-12-20T00:00:02.923426Z"}},"outputs":[{"name":"stdout","text":"Memory usage decreased to 291.26 MB (45.5% reduction)\nStep 2: Data Loaded. Shape: (6362620, 11)\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"   step      type        amount     nameOrig  oldBalanceOrig  newBalanceOrig  \\\n0     1   PAYMENT   9839.639648  C1231006815   170136.000000   160296.359375   \n1     1   PAYMENT   1864.280029  C1666544295    21249.000000    19384.720703   \n2     1  TRANSFER    181.000000  C1305486145      181.000000        0.000000   \n3     1  CASH_OUT    181.000000   C840083671      181.000000        0.000000   \n4     1   PAYMENT  11668.139648  C2048537720    41554.000000    29885.859375   \n5     1   PAYMENT   7817.709961    C90045638    53860.000000    46042.289062   \n6     1   PAYMENT   7107.770020   C154988899   183195.000000   176087.234375   \n7     1   PAYMENT   7861.640137  C1912850431   176087.234375   168225.593750   \n8     1   PAYMENT   4024.360107  C1265012928     2671.000000        0.000000   \n9     1     DEBIT   5337.770020   C712410124    41720.000000    36382.230469   \n\n      nameDest  oldBalanceDest  newBalanceDest  isFraud  isFlaggedFraud  \n0  M1979787155             0.0        0.000000        0               0  \n1  M2044282225             0.0        0.000000        0               0  \n2   C553264065             0.0        0.000000        1               0  \n3    C38997010         21182.0        0.000000        1               0  \n4  M1230701703             0.0        0.000000        0               0  \n5   M573487274             0.0        0.000000        0               0  \n6   M408069119             0.0        0.000000        0               0  \n7   M633326333             0.0        0.000000        0               0  \n8  M1176932104             0.0        0.000000        0               0  \n9   C195600860         41898.0    40348.789062        0               0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>step</th>\n      <th>type</th>\n      <th>amount</th>\n      <th>nameOrig</th>\n      <th>oldBalanceOrig</th>\n      <th>newBalanceOrig</th>\n      <th>nameDest</th>\n      <th>oldBalanceDest</th>\n      <th>newBalanceDest</th>\n      <th>isFraud</th>\n      <th>isFlaggedFraud</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>PAYMENT</td>\n      <td>9839.639648</td>\n      <td>C1231006815</td>\n      <td>170136.000000</td>\n      <td>160296.359375</td>\n      <td>M1979787155</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>PAYMENT</td>\n      <td>1864.280029</td>\n      <td>C1666544295</td>\n      <td>21249.000000</td>\n      <td>19384.720703</td>\n      <td>M2044282225</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>TRANSFER</td>\n      <td>181.000000</td>\n      <td>C1305486145</td>\n      <td>181.000000</td>\n      <td>0.000000</td>\n      <td>C553264065</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>CASH_OUT</td>\n      <td>181.000000</td>\n      <td>C840083671</td>\n      <td>181.000000</td>\n      <td>0.000000</td>\n      <td>C38997010</td>\n      <td>21182.0</td>\n      <td>0.000000</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>PAYMENT</td>\n      <td>11668.139648</td>\n      <td>C2048537720</td>\n      <td>41554.000000</td>\n      <td>29885.859375</td>\n      <td>M1230701703</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>PAYMENT</td>\n      <td>7817.709961</td>\n      <td>C90045638</td>\n      <td>53860.000000</td>\n      <td>46042.289062</td>\n      <td>M573487274</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>PAYMENT</td>\n      <td>7107.770020</td>\n      <td>C154988899</td>\n      <td>183195.000000</td>\n      <td>176087.234375</td>\n      <td>M408069119</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1</td>\n      <td>PAYMENT</td>\n      <td>7861.640137</td>\n      <td>C1912850431</td>\n      <td>176087.234375</td>\n      <td>168225.593750</td>\n      <td>M633326333</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>PAYMENT</td>\n      <td>4024.360107</td>\n      <td>C1265012928</td>\n      <td>2671.000000</td>\n      <td>0.000000</td>\n      <td>M1176932104</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td>DEBIT</td>\n      <td>5337.770020</td>\n      <td>C712410124</td>\n      <td>41720.000000</td>\n      <td>36382.230469</td>\n      <td>C195600860</td>\n      <td>41898.0</td>\n      <td>40348.789062</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"df.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T23:59:24.318938Z","iopub.status.idle":"2025-12-19T23:59:24.319175Z","shell.execute_reply.started":"2025-12-19T23:59:24.319060Z","shell.execute_reply":"2025-12-19T23:59:24.319070Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.groupby('type')['isFraud'].sum()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T23:59:24.319983Z","iopub.status.idle":"2025-12-19T23:59:24.320638Z","shell.execute_reply.started":"2025-12-19T23:59:24.320466Z","shell.execute_reply":"2025-12-19T23:59:24.320483Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.sample(10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T23:59:24.321624Z","iopub.status.idle":"2025-12-19T23:59:24.321916Z","shell.execute_reply.started":"2025-12-19T23:59:24.321766Z","shell.execute_reply":"2025-12-19T23:59:24.321778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe(include='all')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T23:59:24.322628Z","iopub.status.idle":"2025-12-19T23:59:24.322845Z","shell.execute_reply.started":"2025-12-19T23:59:24.322744Z","shell.execute_reply":"2025-12-19T23:59:24.322754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# 1) FEATURE ENGINEER (Fitted inside pipeline)\n# ----------------------------\nclass FraudFeatureEngineer(BaseEstimator, TransformerMixin):\n    \"\"\"Vectorized, deterministic feature transformer.\n    - Builds weighted directed graph (aggregated by (origin,dest) counts)\n    - Creates frequency, ratio, log and graph features\n    \"\"\"\n    def __init__(self, pagerank_limit=None):\n        self.pagerank_limit = pagerank_limit\n        self.stats = {}\n        self.graph_meta = {}\n        self.type_map = {'TRANSFER': 0, 'CASH_OUT': 1}\n        self.global_mean = 0.0\n\n    def fit(self, X, y=None):\n        X = X.copy()\n        # Sort by time if available for deterministic graphs\n        if 'step' in X.columns:\n            X = X.sort_values('step')\n\n        # Basic global stat\n        self.global_mean = float(X['amount'].mean())\n        self.global_median = float(X['amount'].median())\n\n        # Frequency & mean\n        self.stats['orig_counts'] = X['nameOrig'].value_counts().to_dict()\n        self.stats['dest_counts'] = X['nameDest'].value_counts().to_dict()\n        self.stats['orig_mean_amt'] = X.groupby('nameOrig')['amount'].mean().to_dict()\n        self.stats['orig_median_amt'] = (X.groupby('nameOrig')['amount'].median().to_dict())\n        self.stats['orig_log_median_amt'] = (X.groupby('nameOrig')['amount'].apply(lambda s: np.log1p(s).median()).to_dict())\n        self.stats['last_step'] = X.groupby('nameOrig')['step'].last().to_dict()\n\n        # Weighted graph: count transactions per (origin,dest)\n        edge_weights = X.groupby(['nameOrig', 'nameDest']).size().reset_index(name='weight')\n        G = nx.DiGraph()\n        if not edge_weights.empty:\n            edges = [(r['nameOrig'], r['nameDest'], float(r['weight'])) for _, r in edge_weights.iterrows()]\n            G.add_weighted_edges_from(edges, weight='weight')\n\n        self.graph_meta['in_degree'] = dict(G.in_degree(weight='weight'))\n        self.graph_meta['out_degree'] = dict(G.out_degree(weight='weight'))\n\n    \n        # Pagerank: limit nodes if requested to save time/memory\n        try:\n            if G.number_of_nodes() == 0:\n                self.graph_meta['pagerank'] = {}\n            elif self.pagerank_limit and self.pagerank_limit < G.number_of_nodes():\n                top_nodes = sorted(G.degree(weight='weight'), key=lambda x: x[1], reverse=True)[:self.pagerank_limit]\n                keep = set(n for n, _ in top_nodes)\n                sub = G.subgraph(keep).copy()\n                self.graph_meta['pagerank'] = nx.pagerank(sub, alpha=0.85, weight='weight', tol=1e-4)\n            else:\n                self.graph_meta['pagerank'] = nx.pagerank(G, alpha=0.85, weight='weight', tol=1e-4)\n        except Exception:\n            # Pagerank failure should not break training\n            self.graph_meta['pagerank'] = {}\n\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n\n        # Time features\n        X['hour'] = X['step'] % 24 if 'step' in X.columns else 0\n        '''\n        # Balance-check features\n        X['errorBalanceOrig'] = X['newBalanceOrig'].fillna(0) + X['amount'].fillna(0) - X['oldBalanceOrig'].fillna(0)\n        X['errorBalanceDest'] = X['oldBalanceDest'].fillna(0) + X['amount'].fillna(0) - X['newBalanceDest'].fillna(0)\n        '''\n        # Frequency mapping\n        X['orig_txn_count'] = X['nameOrig'].map(self.stats.get('orig_counts', {})).fillna(0).astype(int)\n        X['dest_txn_count'] = X['nameDest'].map(self.stats.get('dest_counts', {})).fillna(0).astype(int)\n\n        # Ratio features\n        user_mean = X['nameOrig'].map(self.stats.get('orig_mean_amt', {})).fillna(self.global_mean)\n        X['amt_ratio_to_user_mean'] = X['amount'] / (user_mean + 1.0)\n        X['amount_log1p'] = np.log1p(X['amount'])\n        X['amount_over_oldBalanceOrig'] = X['amount'] / (X['oldBalanceOrig'].replace(0, np.nan).fillna(1.0))\n        \n        user_median = X['nameOrig'].map(self.stats.get('orig_median_amt', {})).fillna(self.global_median)\n        # Apply fallback to global median for users with too few transactions\n        MIN_TXNS = 3\n        user_median = np.where(X['orig_txn_count'] >= MIN_TXNS, user_median, self.global_median)\n        X['amt_ratio_to_user_median'] = (X['amount'] / (user_median + 1.0))\n        \n        user_log_median = X['nameOrig'].map(self.stats['orig_log_median_amt']).fillna(np.log1p(self.global_median))\n        X['amt_log_ratio_to_user_median'] = (np.log1p(X['amount']) / (user_log_median + 1e-6))\n\n\n        \n        # Graph features\n        X['in_degree'] = X['nameDest'].map(self.graph_meta.get('in_degree', {})).fillna(0).astype(float)\n        X['out_degree'] = X['nameOrig'].map(self.graph_meta.get('out_degree', {})).fillna(0).astype(float)\n        X['network_trust'] = X['nameOrig'].map(self.graph_meta.get('pagerank', {})).fillna(0.0).astype(float)\n\n        # New/novelty flags\n        X['is_new_origin'] = (X['orig_txn_count'] == 0).astype(int)\n        X['is_new_dest'] = (X['dest_txn_count'] == 0).astype(int)\n\n        # Type encoding (fast & vectorized)\n        X['type_encoded'] = X['type'].map(self.type_map).fillna(-1).astype(int)\n\n        # Drop identifiers and non-numeric columns\n        for c in ['nameOrig', 'nameDest', 'type', 'isFlaggedFraud']:\n            if c in X.columns:\n                X.drop(columns=c, inplace=True)\n\n        # Return numeric-only DataFrame expected by XGBoost & SHAP\n        return X.select_dtypes(include=[np.number])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T00:00:05.517839Z","iopub.execute_input":"2025-12-20T00:00:05.518626Z","iopub.status.idle":"2025-12-20T00:00:05.533423Z","shell.execute_reply.started":"2025-12-20T00:00:05.518601Z","shell.execute_reply":"2025-12-20T00:00:05.532607Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# ----------------------------\n# 2) Split helpers: test temporal, train/test only (no validation)\n# ----------------------------\ndef make_splits(df, test_frac=0.10, time_col='step', min_test_fraud=100, random_state=42):\n    \"\"\"\n    Returns: X_train, X_test, y_train, y_test, time_used\n    - Test is time-based (last test_frac proportion) if `time_col` exists.\n    - Otherwise a stratified test split is used.\n    - If temporal test exists but contains < min_test_fraud frauds, fallback to stratified test split.\n    \"\"\"\n    # Temporal test split\n    if time_col in df.columns:\n        cutoff = df[time_col].quantile(1 - test_frac)\n        train_df = df[df[time_col] <= cutoff].reset_index(drop=True)\n        test_df = df[df[time_col] > cutoff].reset_index(drop=True)\n    else:\n        train_df = df.copy()\n        test_df = pd.DataFrame(columns=df.columns)  # empty, fallback\n\n    # If test is too small or contains too few frauds, fallback to stratified split on entire dataset\n    if (test_df.empty) or (test_df['isFraud'].sum() < min_test_fraud):\n        X_temp = df.drop(columns=['isFraud'])\n        y_temp = df['isFraud']\n        X_train, X_test, y_train, y_test = train_test_split(\n            X_temp, y_temp, test_size=test_frac, stratify=y_temp, random_state=random_state\n        )\n        return X_train.reset_index(drop=True), X_test.reset_index(drop=True), y_train.reset_index(drop=True), y_test.reset_index(drop=True), False\n    else:\n        X_train = train_df.drop(columns=['isFraud'])\n        y_train = train_df['isFraud']\n        X_test = test_df.drop(columns=['isFraud'])\n        y_test = test_df['isFraud']\n        return X_train.reset_index(drop=True), X_test.reset_index(drop=True), y_train.reset_index(drop=True), y_test.reset_index(drop=True), True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T00:07:38.644037Z","iopub.execute_input":"2025-12-20T00:07:38.644360Z","iopub.status.idle":"2025-12-20T00:07:38.650961Z","shell.execute_reply.started":"2025-12-20T00:07:38.644341Z","shell.execute_reply":"2025-12-20T00:07:38.650191Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# ----------------------------\n# 3) Training function: train on TRAIN, test on TEST \n# ----------------------------\ndef train_pipeline(df,\n                   pagerank_limit=100000,\n                   random_state=42,\n                   search_iter=3,\n                   target_recall=0.95):\n    \"\"\"\n    Full training flow adapted to only use TRAIN and TEST:\n      - splits data (train / test; temporal test when possible)\n      - randomized search on TRAIN only (with internal CV)\n      - selects threshold using cross-validated predictions on TRAIN\n      - refits final pipeline on TRAIN (no early stopping, since no val)\n      - returns final_pipeline, optimal_threshold, and test set for evaluation\n    \"\"\"\n    # Prepare splits (now returns X_train, X_test, y_train, y_test, time_used)\n    X_train, X_test, y_train, y_test, time_used = make_splits(\n        df, test_frac=0.05, time_col='step', random_state=random_state\n    )\n\n    print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}; time_split_used={time_used}\")\n    print(\"Fraud counts -> train:\", int(y_train.sum()), \"test:\", int(y_test.sum()))\n\n\n    # Build pipeline objects\n    fe = FraudFeatureEngineer(pagerank_limit=pagerank_limit)\n    #-----trnsOutSrch\n    print(\"\\n[Step A] Running Feature Engineering on FULL Training Set (Once)...\") \n    # Fit and transform the raw X_train once\n    X_train_search = fe.fit_transform(X_train, y_train)\n    #-----trnsOutSrch\n    xgb_base = xgb.XGBClassifier(objective='binary:logistic',\n                                 tree_method=\"hist\",\n                                 device=device,\n                                 n_jobs=1,  # keep job control to outer search\n                                 random_state=random_state,\n                                 use_label_encoder=False,\n                                 verbosity=1)\n\n    pipeline = Pipeline([('fe', fe), ('clf', xgb_base)])\n\n    # Heuristic scale_pos_weight (from train)\n    neg = (y_train == 0).sum()\n    pos = (y_train == 1).sum()\n    spw = int(max(1, neg / max(1, pos)))  #scale_pos_weight\n\n    param_dist = {\n        'n_estimators': randint(300, 900),\n        'max_depth': randint(4, 8),\n        'learning_rate': uniform(0.03, 0.12),\n        'scale_pos_weight': [spw],\n        'subsample': uniform(0.7, 0.3),\n        'colsample_bytree': uniform(0.6, 0.4)\n    }\n    \n    \"\"\"\n    param_dist = {\n    # Number of trees\n    'clf__n_estimators': randint(300, 900),\n    # Tree depth (PaySim works best shallow–medium)\n    'clf__max_depth': randint(4, 8),\n    # Learning rate (avoid very small, too slow)\n    'clf__learning_rate': uniform(0.03, 0.12),\n    # Imbalance handling (keep fixed or narrow)\n    'clf__scale_pos_weight': [spw],\n    # Row sampling (important for generalization)\n    'clf__subsample': uniform(0.7, 0.3),\n    # Column sampling (often overlooked, very important)\n    'clf__colsample_bytree': uniform(0.6, 0.4)\n    }\n    \"\"\"\n    ######\n    ####\n    ###-----trnsOutSrch srch optmize .  pipeline --> xgb_base  X_train --> X_train_search\n    search = RandomizedSearchCV(xgb_base, param_distributions=param_dist, n_iter=search_iter,\n                                scoring='average_precision', cv=3, verbose=2, n_jobs=-1, random_state=random_state,\n                                refit=True)\n    # Fit search on TRAIN only\n    print(f\"\\nStarting RandomizedSearchCV: {search_iter} candidates × 3 folds = {search_iter * 3} model fits\")\n    print(\"This may take several minutes depending on dataset size. Progress will be shown below...\\n\")\n    search.fit(X_train_search, y_train)\n    print(\"\\n\\nRandom search best params:\", search.best_params_)\n\n    \n    # ---------- Threshold selection using cross-validated predictions on TRAIN ----------\n    # Use cross_val_predict to get out-of-fold probabilities on training data (keeps test untouched)\n    try:\n        train_probs_cv = cross_val_predict(\n            search.best_estimator_, X_train_search, y_train, cv=3,\n            method='predict_proba', n_jobs=2\n        )[:, 1]\n    except Exception:\n        # Fallback: use the fitted pipeline's predict_proba on X_train (risk of optimistic threshold)\n        train_probs_cv = search.predict_proba(X_train_search)[:, 1]\n\n    precision, recall, thresholds = precision_recall_curve(y_train, train_probs_cv)\n    idxs = np.where(recall[:-1] >= target_recall)[0]\n    if len(idxs) > 0:\n        optimal_threshold = float(thresholds[idxs[-1]])  # largest threshold preserving recall\n    else:\n        optimal_threshold = float(thresholds[0]) if len(thresholds) > 0 else 0.2\n\n    print(f\"Chosen threshold from TRAIN (CV) : {optimal_threshold:.8f} (target recall {target_recall})\")\n\n    # Optionally print training performance at chosen threshold (out-of-fold)\n    train_preds_cv = (train_probs_cv >= optimal_threshold).astype(int)\n    print(\"\\nTraining (CV) performance at chosen threshold:\")\n    print(classification_report(y_train, train_preds_cv))\n    print(\"Train Confusion Matrix:\\n\", confusion_matrix(y_train, train_preds_cv))\n\n    # ---------- Refit final model on full TRAIN ----------\n    # Fit a fresh feature engineer on entire train\n    fe_final = FraudFeatureEngineer(pagerank_limit=pagerank_limit)\n    fe_final.fit(X_train, y_train)\n\n    X_train_trans = fe_final.transform(X_train)\n    #X_test_trans = fe_final.transform(X_test) if len(X_test) > 0 else None\n    print(\"\\nSample of X_train_trans:\")\n    print(X_train_trans.sample(10))\n\n\n    # Build classifier with best params (no early stopping since no val set)\n    #cls_params = {k.replace('clf__', ''): v for k, v in search.best_params_.items() if k.startswith('clf__')}\n    cls_params =search.best_params_\n    clf_final = xgb.XGBClassifier(objective='binary:logistic',\n                                  tree_method=\"hist\",\n                                  device=device,\n                                  n_jobs=1,\n                                  random_state=random_state,\n                                  use_label_encoder=False,\n                                  verbosity=2,\n                                  **cls_params)\n\n    clf_final.fit(X_train_trans, y_train)\n\n    # Final pipeline contains fitted FE and fitted classifier\n    final_pipeline = Pipeline([('fe', fe_final), ('clf', clf_final)])\n    #\n    ##----refit=True\n    ###final_pipeline = Pipeline([('fe', fe), ('clf', search.best_estimator_)])\n    \n    # Save pipeline artifact\n    joblib.dump(final_pipeline, 'fraud_pipeline_final.pkl')\n    print(\"Saved final pipeline to 'fraud_pipeline_final.pkl'\")\n\n    # Return pipeline, threshold and test partitions (raw df forms for later inference)\n    return final_pipeline, optimal_threshold, X_test, y_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T00:08:51.710737Z","iopub.execute_input":"2025-12-20T00:08:51.711034Z","iopub.status.idle":"2025-12-20T00:08:51.723873Z","shell.execute_reply.started":"2025-12-20T00:08:51.711011Z","shell.execute_reply":"2025-12-20T00:08:51.723017Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# ----------------------------\n# 4) SHAP + LLM explain function and prediction wrapper\n# ----------------------------\ndef predict_and_explain(pipeline, transaction_df, threshold, shap_background=None, topk=6, llm_api_key=None):\n    \"\"\"\n    - pipeline: fitted sklearn Pipeline with steps 'fe' and 'clf'\n    - transaction_df: raw transaction(s) DataFrame (same columns used in training)\n    - threshold: numeric threshold chosen from validation\n    - shap_background: optional background DataFrame for SHAP explainer (raw, not transformed).\n                       If None, the function will attempt to sample from pipeline.fe.stats if available.\n    - topk: how many top contributors to include in the explanation\n    - llm_api_key: if provided, function will call Groq API to generate a textual explanation.\n\n    Returns: dict with keys:\n      'probabilities', 'decisions', 'shap_table' (DataFrame), 'llm_explanation' (str or None)\n    \"\"\"\n    # Predict probabilities & binary decision\n    probs = pipeline.predict_proba(transaction_df)[:, 1]\n    decisions = (probs >= threshold).astype(int)\n\n    # Prepare features for SHAP (transformed numeric features)\n    fe = pipeline.named_steps['fe']\n    clf = pipeline.named_steps['clf']\n\n    X_trans = fe.transform(transaction_df)  # numeric matrix with columns in fixed order\n    # Determine background for SHAP: prefer provided, else use fe.stats to synthesize small background\n    if shap_background is None:\n        # If we have access to saved stats, try to build a tiny synthetic background sample.\n        # Fallback: use the transaction itself repeated (not ideal but safe).\n        try:\n            # if fe.stats contains 'orig_counts', we might not have raw rows; fallback above\n            shap_background_trans = X_trans.iloc[[0]].copy()\n        except Exception:\n            shap_background_trans = X_trans.iloc[[0]].copy()\n    else:\n        # transform provided background raw df\n        shap_background_trans = fe.transform(shap_background)\n\n    # Create SHAP explainer\n    try:\n        # shap.Explainer is model-agnostic and often handles sklearn wrappers well\n        explainer = shap.Explainer(clf, shap_background_trans, feature_names=X_trans.columns.tolist())\n        shap_exp = explainer(X_trans)  # Explanation object\n        # shap_exp.values shape: (n_samples, n_features)\n        shap_vals = shap_exp.values[0] if shap_exp.values.ndim == 2 else shap_exp.values\n        feature_names = X_trans.columns.tolist()\n    except Exception:\n        # Last-resort fallback using TreeExplainer on the raw booster - works for xgboost\n        try:\n            explainer = shap.TreeExplainer(clf)\n            shap_vals = explainer.shap_values(X_trans)[0]\n            feature_names = X_trans.columns.tolist()\n        except Exception:\n            # Unable to compute SHAP; return empty table\n            shap_vals = np.zeros(X_trans.shape[1])\n            feature_names = X_trans.columns.tolist()\n\n    # Build a DataFrame of feature contributions\n    feat_df = pd.DataFrame({\n        'feature': feature_names,\n        'value': X_trans.iloc[0].values,\n        'shap_abs': np.abs(shap_vals),\n        'shap': shap_vals\n    })\n    feat_df = feat_df.sort_values('shap_abs', ascending=False).reset_index(drop=True)\n\n    # Prepare LLM prompt summarizing top contributors\n    top_feats = feat_df.head(topk)[['feature', 'value', 'shap']].copy()\n    top_lines = []\n    for _, row in top_feats.iterrows():\n        top_lines.append(f\"- {row['feature']}: value={row['value']:.6g}, shap={row['shap']:.6g}\")\n\n    llm_explanation_text = None\n    if llm_api_key:\n        try:\n            from groq import Groq  # <--- CHANGED: Import Groq instead of openai\n            \n            client = Groq(api_key=llm_api_key) # <--- CHANGED: Initialize Groq client\n            \n            system_prompt = (\n                \"You are a concise fraud-analytics assistant. \"\n                \"Given feature contributions (SHAP values) and their observed values for a single transaction, \"\n                \"produce a short (3-6 sentences) human-readable explanation why the model assigned the given fraud probability. \"\n                \"Mention which factors increase or decrease risk, and a brief recommended action (e.g., block / review / allow).\"\n            )\n            user_prompt = (\n                f\"Model fraud probability: {probs[0]:.4f}\\n\"\n                f\"Threshold for blocking: {threshold:.4f}\\n\"\n                f\"Top contributing features (feature: value, shap):\\n\" + \"\\n\".join(top_lines) +\n                \"\\n\\nWrite the short explanation now.\"\n            )\n\n            # <--- CHANGED: Call Groq's chat.completions with a free Llama 3 model\n            chat_completion = client.chat.completions.create(\n                messages=[\n                    {\"role\": \"system\", \"content\": system_prompt},\n                    {\"role\": \"user\", \"content\": user_prompt}\n                ],\n                model = \"llama-3.1-8b-instant\"  , # <--- CHANGED: Use free 'llama3-8b-8192' model\n                temperature=0.0,\n                max_tokens=250\n            )\n            \n            # <--- CHANGED: Access response content (structure is same as OpenAI)\n            llm_explanation_text = chat_completion.choices[0].message.content.strip()\n            \n        except Exception as e:\n            llm_explanation_text = f\"(LLM generation failed: {str(e)})\"\n    # Return structured outputs\n    return {\n        'probabilities': probs,\n        'decisions': decisions,\n        'shap_table': feat_df,\n        'llm_explanation': llm_explanation_text\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T00:08:59.484613Z","iopub.execute_input":"2025-12-20T00:08:59.485125Z","iopub.status.idle":"2025-12-20T00:08:59.495726Z","shell.execute_reply.started":"2025-12-20T00:08:59.485102Z","shell.execute_reply":"2025-12-20T00:08:59.494681Z"},"_kg_hide-input":false},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# ----------------------------\n# 5) Example usage: train, then predict and explain on one sample\n# ----------------------------\nif __name__ == \"__main__\":\n    # Load dataset (adjust path as needed)\n    # Adjust path if necessary.\n    file_path = '/kaggle/input/paysim1/PS_20174392719_1491204439457_log.csv'\n\n    # Read CSV\n    df = pd.read_csv(file_path)\n    df = reduce_mem_usage(df)\n    df = df.rename(columns={'oldbalanceOrg':'oldBalanceOrig', 'newbalanceOrig':'newBalanceOrig',\n                            'oldbalanceDest':'oldBalanceDest', 'newbalanceDest':'newBalanceDest'})\n    df = df[df['type'].isin(['TRANSFER', 'CASH_OUT'])].reset_index(drop=True)\n\n    # Train pipeline (this will take some time depending on data size)\n    pipeline, threshold, X_test_raw, y_test = train_pipeline(df, pagerank_limit=100000,\n                                                            random_state=42, search_iter=6,\n                                                            target_recall=0.95)\n\n    # Example single new transaction (raw columns same as training raw df)\n    new_transaction = pd.DataFrame([{\n        'step': df['step'].max() + 1,\n        'type': 'CASH_OUT',\n        'amount': 500000.0,\n        'nameOrig': 'C12345_NEW_USER',\n        'oldBalanceOrig': 500000.0,\n        'newBalanceOrig': 0.0,\n        'nameDest': 'C99999_EXISTING_BAD',\n        'oldBalanceDest': 0.0,\n        'newBalanceDest': 0.0,\n        'isFlaggedFraud': 0\n    }])\n\n    # Optionally supply a small background sample for SHAP (raw rows from training)\n    # Here we sample 200 rows from the original training area if available\n    shap_bg = None\n    try:\n        # if we have a test partition raw DataFrame, use some rows from it as background (or from df earlier)\n        shap_bg = df.sample(n=200, random_state=42).drop(columns=['isFraud'])\n    except Exception:\n        shap_bg = None\n\n    # If you want LLM textual explanations, provide OPENAI_API_KEY environment variable or pass into function\n    GROQ_API_KEY = os.getenv('GROQ_API_KEY', \"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\")\n\n    # Predict and explain\n    # <--- CHANGED: Pass 'llm_api_key' instead of 'openai_api_key'\n    result = predict_and_explain(pipeline, new_transaction, threshold, shap_background=shap_bg,\n                                 topk=6, llm_api_key=GROQ_API_KEY)\n    \n    print(f\"Fraud probability: {result['probabilities'][0]:.4f}\")\n    print(f\"Decision (threshold {threshold:.4f}): {'BLOCK' if result['decisions'][0] == 1 else 'ALLOW'}\")\n    print(\"\\nTop SHAP contributors:\")\n    print(result['shap_table'].head(10).to_string(index=False))\n\n    if result['llm_explanation']:\n        print(\"\\nLLM Explanation:\")\n        print(result['llm_explanation'])\n    else:\n        print(\"\\nNo LLM explanation generated (provide GROQ_API_KEY environment variable to enable).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T00:21:53.809808Z","iopub.execute_input":"2025-12-20T00:21:53.810508Z","iopub.status.idle":"2025-12-20T00:40:56.827437Z","shell.execute_reply.started":"2025-12-20T00:21:53.810484Z","shell.execute_reply":"2025-12-20T00:40:56.826582Z"},"_kg_hide-input":false},"outputs":[{"name":"stdout","text":"Memory usage decreased to 291.26 MB (45.5% reduction)\nTrain size: 2632630, Test size: 137779; time_split_used=True\nFraud counts -> train: 5275 test: 2938\n\n[Step A] Running Feature Engineering on FULL Training Set (Once)...\n\nStarting RandomizedSearchCV: 6 candidates × 3 folds = 18 model fits\nThis may take several minutes depending on dataset size. Progress will be shown below...\n\nFitting 3 folds for each of 6 candidates, totalling 18 fits\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [00:29:16] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [00:29:16] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [00:29:16] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [00:29:22] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"\n\nRandom search best params: {'colsample_bytree': 0.7599443886861021, 'learning_rate': 0.03559987958563385, 'max_depth': 7, 'n_estimators': 489, 'scale_pos_weight': 498, 'subsample': 0.7271819303598462}\n[CV] END colsample_bytree=0.749816047538945, learning_rate=0.14408571676918994, max_depth=6, n_estimators=371, scale_pos_weight=498, subsample=0.8795975452591109; total time=  22.9s\n[CV] END colsample_bytree=0.6624074561769746, learning_rate=0.048719342440344315, max_depth=6, n_estimators=758, scale_pos_weight=498, subsample=0.9598528437324805; total time=  31.1s\n[CV] END colsample_bytree=0.9329770563201687, learning_rate=0.055480693281393136, max_depth=7, n_estimators=576, scale_pos_weight=498, subsample=0.8852444528883149; total time=  28.0s\n[CV] END colsample_bytree=0.8446612641953124, learning_rate=0.030847956626366087, max_depth=4, n_estimators=860, scale_pos_weight=498, subsample=0.8574323980775167; total time=  29.1s\n[CV] END colsample_bytree=0.6624074561769746, learning_rate=0.048719342440344315, max_depth=6, n_estimators=758, scale_pos_weight=498, subsample=0.9598528437324805; total time=  28.7s\n[CV] END colsample_bytree=0.8404460046972835, learning_rate=0.11496870933552546, max_depth=5, n_estimators=608, scale_pos_weight=498, subsample=0.9909729556485982; total time=  26.5s\n[CV] END colsample_bytree=0.9329770563201687, learning_rate=0.055480693281393136, max_depth=7, n_estimators=576, scale_pos_weight=498, subsample=0.8852444528883149; total time=  29.2s\n[CV] END colsample_bytree=0.8446612641953124, learning_rate=0.030847956626366087, max_depth=4, n_estimators=860, scale_pos_weight=498, subsample=0.8574323980775167; total time=  29.7s\n[CV] END colsample_bytree=0.749816047538945, learning_rate=0.14408571676918994, max_depth=6, n_estimators=371, scale_pos_weight=498, subsample=0.8795975452591109; total time=  23.2s\n[CV] END colsample_bytree=0.8404460046972835, learning_rate=0.11496870933552546, max_depth=5, n_estimators=608, scale_pos_weight=498, subsample=0.9909729556485982; total time=  27.6s\n[CV] END colsample_bytree=0.8404460046972835, learning_rate=0.11496870933552546, max_depth=5, n_estimators=608, scale_pos_weight=498, subsample=0.9909729556485982; total time=  22.5s\n[CV] END colsample_bytree=0.8446612641953124, learning_rate=0.030847956626366087, max_depth=4, n_estimators=860, scale_pos_weight=498, subsample=0.8574323980775167; total time=  23.4s\n[CV] END colsample_bytree=0.7599443886861021, learning_rate=0.03559987958563385, max_depth=7, n_estimators=489, scale_pos_weight=498, subsample=0.7271819303598462; total time=  19.9s\n[CV] END colsample_bytree=0.749816047538945, learning_rate=0.14408571676918994, max_depth=6, n_estimators=371, scale_pos_weight=498, subsample=0.8795975452591109; total time=  23.1s\n[CV] END colsample_bytree=0.6624074561769746, learning_rate=0.048719342440344315, max_depth=6, n_estimators=758, scale_pos_weight=498, subsample=0.9598528437324805; total time=  32.3s\n[CV] END colsample_bytree=0.9329770563201687, learning_rate=0.055480693281393136, max_depth=7, n_estimators=576, scale_pos_weight=498, subsample=0.8852444528883149; total time=  29.3s\n[CV] END colsample_bytree=0.7599443886861021, learning_rate=0.03559987958563385, max_depth=7, n_estimators=489, scale_pos_weight=498, subsample=0.7271819303598462; total time=  24.6s\n[CV] END colsample_bytree=0.7599443886861021, learning_rate=0.03559987958563385, max_depth=7, n_estimators=489, scale_pos_weight=498, subsample=0.7271819303598462; total time=  14.1s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [00:31:31] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n/usr/local/lib/python3.11/dist-packages/xgboost/core.py:160: UserWarning: [00:31:31] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Chosen threshold from TRAIN (CV) : 0.12684587 (target recall 0.95)\n\nTraining (CV) performance at chosen threshold:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00   2627355\n           1       0.52      0.95      0.67      5275\n\n    accuracy                           1.00   2632630\n   macro avg       0.76      0.97      0.83   2632630\nweighted avg       1.00      1.00      1.00   2632630\n\nTrain Confusion Matrix:\n [[2622668    4687]\n [    263    5012]]\n\nSample of X_train_trans:\n         step        amount  oldBalanceOrig  newBalanceOrig  oldBalanceDest  \\\n241177     21  2.254583e+05        0.000000        0.000000    9.688909e+05   \n813357    163  9.987603e+04   121968.679688    22092.650391    8.517879e+05   \n898952    181  2.177635e+05     2144.000000        0.000000    0.000000e+00   \n1115321   206  3.778693e+05        0.000000        0.000000    1.038409e+06   \n277956     35  2.280544e+05   324550.000000    96495.562500    2.216491e+05   \n1894891   308  4.093195e+05      151.000000        0.000000    0.000000e+00   \n203783     19  1.814813e+05        0.000000        0.000000    2.478744e+06   \n677562    154  1.171912e+06    64841.000000        0.000000    7.923618e+05   \n410290     43  1.675642e+05    35846.000000        0.000000    1.342485e+06   \n2314594   373  8.901410e+05        0.000000        0.000000    1.504731e+06   \n\n         newBalanceDest  hour  orig_txn_count  dest_txn_count  \\\n241177     1.194349e+06    21               1              15   \n813357     9.516639e+05    19               1              13   \n898952     2.177635e+05    13               1               7   \n1115321    1.416278e+06    14               1              14   \n277956     4.497036e+05    11               1              20   \n1894891    4.093195e+05    20               1               2   \n203783     2.660226e+06    19               1              27   \n677562     1.964274e+06    10               1              14   \n410290     1.510049e+06    19               1               5   \n2314594    2.394872e+06    13               1               8   \n\n         amt_ratio_to_user_mean  amount_log1p  amount_over_oldBalanceOrig  \\\n241177                 0.999996     12.325895               225458.312500   \n813357                 0.999990     11.511695                    0.818866   \n898952                 0.999995     12.291169                  101.568802   \n1115321                0.999997     12.842306               377869.343750   \n277956                 0.999996     12.337344                    0.702679   \n1894891                0.999998     12.922254                 2710.725342   \n203783                 0.999994     12.108913               181481.265625   \n677562                 0.999999     13.974149                   18.073633   \n410290                 0.999994     12.029128                    4.674558   \n2314594                0.999999     13.699137               890141.000000   \n\n         amt_ratio_to_user_median  amt_log_ratio_to_user_median  in_degree  \\\n241177                   1.314684                           1.0       15.0   \n813357                   0.582394                           1.0       13.0   \n898952                   1.269815                           1.0        7.0   \n1115321                  2.203418                           1.0       14.0   \n277956                   1.329823                           1.0       20.0   \n1894891                  2.386809                           1.0        2.0   \n203783                   1.058247                           1.0       27.0   \n677562                   6.833614                           1.0       14.0   \n410290                   0.977094                           1.0        5.0   \n2314594                  5.190558                           1.0        8.0   \n\n         out_degree  network_trust  is_new_origin  is_new_dest  type_encoded  \n241177          1.0            0.0              0            0             1  \n813357          1.0            0.0              0            0             1  \n898952          1.0            0.0              0            0             1  \n1115321         1.0            0.0              0            0             1  \n277956          1.0            0.0              0            0             1  \n1894891         1.0            0.0              0            0             0  \n203783          1.0            0.0              0            0             1  \n677562          1.0            0.0              0            0             0  \n410290          1.0            0.0              0            0             1  \n2314594         1.0            0.0              0            0             0  \nSaved final pipeline to 'fraud_pipeline_final.pkl'\n","output_type":"stream"},{"name":"stderr","text":"[00:40:20] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n","output_type":"stream"},{"name":"stdout","text":"Fraud probability: 0.9999\nDecision (threshold 0.1268): BLOCK\n\nTop SHAP contributors:\n                   feature         value  shap_abs      shap\namount_over_oldBalanceOrig      1.000000 11.421242 11.421242\n            oldBalanceOrig 500000.000000  3.794964  3.794964\n            oldBalanceDest      0.000000  2.329231  2.329231\n                      hour      0.000000  1.415768  1.415768\n                      step    744.000000  0.591295  0.591295\n            newBalanceOrig      0.000000  0.524133  0.524133\n                    amount 500000.000000  0.509880  0.509880\n            dest_txn_count      0.000000  0.166954  0.166954\n              type_encoded      1.000000  0.114061  0.114061\n              amount_log1p     13.122365  0.081734  0.081734\n\nLLM Explanation:\nThe model assigned a high fraud probability of 0.9999, indicating a strong likelihood of a fraudulent transaction. The top contributing factors are:\n\n- A large transaction amount (1) compared to the old balance (500,000), increasing risk.\n- The old balance is significantly high, suggesting a large account with a substantial amount of money, which may be a target for fraud.\n- The transaction occurred at midnight (hour=0), which is an unusual time for legitimate transactions, increasing risk.\n- The transaction step (744) is high, indicating a potentially suspicious sequence of events.\n\nRecommended action: Block the transaction due to the extremely high fraud probability and suspicious patterns.\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"import joblib\nX_train, X_test_raw, y_train, y_test, time_used = make_splits(\n        df, test_frac=0.05, time_col='step', random_state=random_state)\nGROQ_API_KEY = os.getenv('GROQ_API_KEY', \"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\")\npipeline = joblib.load(\"/kaggle/input/frddtctmodel1/fraud_pipeline_final (1).pkl\")\n\nthreshold = 0.0793  # reuse training threshold\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T23:59:24.330238Z","iopub.status.idle":"2025-12-19T23:59:24.330724Z","shell.execute_reply.started":"2025-12-19T23:59:24.330609Z","shell.execute_reply":"2025-12-19T23:59:24.330620Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result = predict_and_explain(pipeline, X_test_raw, threshold, shap_background=None,topk=4, llm_api_key=GROQ_API_KEY)\nprint(\"\\nTEST performance at TRAIN-chosen threshold:\")\nprint(classification_report(y_test, result['decisions']))\nprint(\"Test Confusion Matrix:\\n\",confusion_matrix(y_test, result['decisions']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-20T01:03:18.803822Z","iopub.execute_input":"2025-12-20T01:03:18.804137Z","iopub.status.idle":"2025-12-20T01:04:20.034577Z","shell.execute_reply.started":"2025-12-20T01:03:18.804116Z","shell.execute_reply":"2025-12-20T01:04:20.033685Z"}},"outputs":[{"name":"stderr","text":" 98%|===================| 134872/137779 [00:20<00:00]        ","output_type":"stream"},{"name":"stdout","text":"\nTEST performance at TRAIN-chosen threshold:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00    134841\n           1       0.92      1.00      0.96      2938\n\n    accuracy                           1.00    137779\n   macro avg       0.96      1.00      0.98    137779\nweighted avg       1.00      1.00      1.00    137779\n\nTest Confusion Matrix:\n [[134586    255]\n [     0   2938]]\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"!ls -lh /kaggle/working\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-19T23:59:24.334551Z","iopub.status.idle":"2025-12-19T23:59:24.334852Z","shell.execute_reply.started":"2025-12-19T23:59:24.334694Z","shell.execute_reply":"2025-12-19T23:59:24.334708Z"}},"outputs":[],"execution_count":null}]}